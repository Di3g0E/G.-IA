{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apuntes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías:\n",
    "import pandas as pd             # Manejo de datos\n",
    "import numpy as np              # Manejo de arrays n-dimensionales y func. matemáticas\n",
    "import scipy                    # Manejo de func. matemáticas y distrib. de probabilidad\n",
    "from sklearn.preprocessing import PolynomialFeatures, MinMaxScaler, MaxAbsScaler, StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import random                   # Generación aleatoria\n",
    "import matplotlib.pyplot as plt # Generación de gráficos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base de datos de prueba\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Abrir la Base de Datos: ###\n",
    "\n",
    "# bd = pd.read_csv('C:/Users/Diego/OneDrive - Universidad Rey Juan Carlos/Documentos/GIA_URJC/Curso 2023-24/G.-IA/G.-IA/Curso_2/Cuatri_2/AprendizajeAutomatico_1/Apuntes/california_housing_apuntes.csv', sep=',')\n",
    "\n",
    "# Base de datos de prueba:\n",
    "iris_data = load_iris()\n",
    "bd = pd.DataFrame(iris_data.data, columns=iris_data.feature_names)\n",
    "bd['species'] = pd.Categorical.from_codes(iris_data.target, iris_data.target_names)\n",
    "\n",
    "# Guardar tamaño de bd:\n",
    "N, D = bd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dividir en Train-Test: ###\n",
    "\n",
    "fraction_test = 0.2\n",
    "\n",
    "# Crea una lista con los índices de la bd:\n",
    "ind = bd.index.tolist()\n",
    "# Desordena los índices:\n",
    "random.shuffle(ind)\n",
    "\n",
    "# Calcula la cantidad de ejemplos que se guardan en test:\n",
    "N_test = int(N * fraction_test)\n",
    "\n",
    "# Divide los datos:\n",
    "test_df = bd.iloc[ind[:N_test]]\n",
    "train_df = bd.iloc[ind[N_test:]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprime el tamaño de las bd:\n",
    "print(f'BD size: ({N}, {D})\\nTrain size: {train_df.shape} \\t Test size: {test_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar los datos de train y Test en un directorio:\n",
    "\n",
    "# flag_save = True\n",
    "# if flag_save:\n",
    "#     train_folder = 'C:/Users/Diego/OneDrive - Universidad Rey Juan Carlos/Documentos/GIA_URJC/Curso 2023-24/G.-IA/G.-IA/Curso_2/Cuatri_2/AprendizajeAutomatico_1/Apuntes/'\n",
    "#     train_name   = 'train_df_apuntes.csv'\n",
    "#     train_df.to_csv(train_folder + train_name, sep=';', header=True)\n",
    "\n",
    "# if flag_save:\n",
    "#     test_folder = 'C:/Users/Diego/OneDrive - Universidad Rey Juan Carlos/Documentos/GIA_URJC/Curso 2023-24/G.-IA/G.-IA/Curso_2/Cuatri_2/AprendizajeAutomatico_1/Apuntes/'\n",
    "#     test_name   = 'test_apuntes.csv'\n",
    "#     test_df.to_csv(test_folder + test_name, sep=';', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Información de los datos para ver si hay que codificar:\n",
    "train_df.info()\n",
    "\n",
    "### Tipos de datos: Atributos 'continuos', 'discretos' y 'categóricos' ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### La codificación de los tipos categóricos no siempre hay que hacerla por la falta de estos###\n",
    "# 1) Averiguamos las columnas categóricas:\n",
    "cat_cols = train_df.select_dtypes(include='category').columns.tolist()\n",
    "\n",
    "# 2) Creamos un dataframe con las columnas categóricas:\n",
    "train_cod = train_df\n",
    "\n",
    "# 3) Codificamos la fila, a la vez que creamos una diccionario de diccionarios para descodificar en el futuro\n",
    "dict_decode={}\n",
    "for col in cat_cols:\n",
    "  codes = train_df[col].cat.codes                   # Codifica la serie del df actual\n",
    "  code_to_categ = dict(zip(codes,train_df[col]))    # Crea la relación de codificación\n",
    "  train_cod[col] = codes                            # Asigna el valor codificado a la serie\n",
    "  dict_decode[col] = code_to_categ                  # Añade la decodificación al diccionario\n",
    "\n",
    "print(f'Columnas categóricas: {cat_cols}\\n{train_cod.head()}\\nDecodificación: {dict_decode}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tambien se puede codificar por One-Hot. Elegir cómo nos viene mejor. ###\n",
    "### Codificación One-hot: crea una columna por cada categoría validando con 1 y 0 ###\n",
    "OneHot_codification = train_df\n",
    "for col in cat_cols:\n",
    "    OneHot_codification = pd.concat([OneHot_codification, pd.get_dummies(OneHot_codification[col])], axis=1)\n",
    "\n",
    "OneHot_codification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver la información del df\n",
    "print(train_cod.info(), '\\n')        # Indica el tamaño, los tipos de datos y cuántos son 'Na' de cada atributo\n",
    "print(train_cod.describe(), '\\n')    # Indica la descripción estadística básica excepto la moda de cada atributo en forma de tabla\n",
    "train_cod.mode(axis=0, dropna=False) # Devuelve una tabla con las modas de cada atributo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Búsqueda de valores NaN:\n",
    "missing_data = train_cod.isna()\n",
    "missing_data.sum()\n",
    "\n",
    "missing_values_per_column = missing_data.sum(axis=0)    # 'NA' por cada columna\n",
    "missing_values_per_row = missing_data.sum(axis=1)       # 'NA' por cada fila\n",
    "\n",
    "mask_mayorq0 = missing_values_per_column > 0            # Crea una máscara de Pandas para indicar si hay columnas con NA\n",
    "mask_mayorq1 = missing_values_per_row > 0               # Crea una máscara de Pandas para indicar si hay filas con NA\n",
    "\n",
    "print(f'Columnas con valores nulos:\\n{missing_values_per_column[mask_mayorq0]}\\n')\n",
    "print(f'Filas con valores nulos:\\n{missing_values_per_row[mask_mayorq1]}\\n')\n",
    "\n",
    "missing_count_row = missing_values_per_row.value_counts().sort_index()\n",
    "print(f'Valores NaN en cada fila:\\n{missing_count_row}')\n",
    "missing_count_col = missing_values_per_column.value_counts().sort_index()\n",
    "print(f'Valores NaN en cada columna:\\n{missing_count_col}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Se puede hacer una de las siguientes cosas:\n",
    "- Eliminar filas/columnas Nan\n",
    "- Imputación univariada\n",
    "- Imputación multivariante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Eliminar filas y columnas con NaN excesivos ###\n",
    "\n",
    "# Eliminar filas:\n",
    "if missing_count_row.index[-1] > 0:                                         # Calcular la cantidad máxima de valores nulos por fila\n",
    "    mask_toDrop = missing_values_per_row >= missing_count_row.index[-1]     # Filtro que busca las filas con el número de valores perdidos máximo\n",
    "    drop_list_row = missing_values_per_row[mask_toDrop].index.tolist()      # Crea una lista de índices de las filas que cumplen con la condición\n",
    "    train_cod.drop(drop_list_row, inplace=True)                             # Eliminar las filas guardadas en 'drop_list' del DataFrame original\n",
    "\n",
    "# Eliminar columnas:                                      \n",
    "if missing_count_col.index[-1] > 0:                                     # Calcular la cantidad máxima de valores nulos por columna\n",
    "    mask_toDrop = missing_values_per_column >= missing_count_col.index[-1]  # Filtro que busca las filas con el número de valores perdidos máximo\n",
    "    drop_list_col = missing_values_per_column[mask_toDrop].index.tolist()   # Crea una lista de índices de las columnas que cumplen con la condición\n",
    "    train_cod.drop(drop_list_col, inplace=True)                             # Eliminar las columnas guardadas en 'drop_list' del DataFrame original, al usar implace no hace falta poner 'df = ...'\n",
    "\n",
    "# Tamaño del DataFrame redimensionado:\n",
    "train_cod.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imputación univariada de los datos NaN: ###\n",
    "\n",
    "train_cod.fillna(train_cod.median(axis=0), inplace=True)    # Sustituye los valores NaN por el valore de la mediana\n",
    "train_cod.shape\n",
    "# Se pueden sustituir por la media o el valor más repetido (moda)\n",
    "# El problema de esta técnica es que puede crear ejemplos imposibles como decir que un hombre está embarazado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imputación multivariante de los datos NaN: ###\n",
    "\n",
    "imputer = IterativeImputer()\n",
    "train_imputed = imputer.fit_transform(train_cod)\n",
    "\n",
    "# Convertir de nuevo a DataFrame\n",
    "train_imputed = pd.DataFrame(train_imputed, columns=train_cod.columns)\n",
    "\n",
    "# Recuento de NaN:\n",
    "train_imputed.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ingeniería de características ###\n",
    "\n",
    "# Crea una tabla con las correlaciones de las diferentes variables:\n",
    "correlation_matrix = train_imputed.corr()\n",
    "\n",
    "# Aumento de la dimensionalidad de las variables con baja correlación:\n",
    "train_df_dim = train_imputed\n",
    "degree = 2\n",
    "interaction_only = True\n",
    "\n",
    "polyf = PolynomialFeatures(degree=degree, interaction_only=interaction_only, include_bias=False)\n",
    "polyf.set_output(transform=\"pandas\")\n",
    "\n",
    "\n",
    "# No se puede hacer esta línea de comando con el test NUNCA:\n",
    "polyf.fit(train_cod)\n",
    "# Adapta los datos a las nuevas características:\n",
    "train_df_dim = polyf.transform(train_cod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Dataframe aumentado:\\n{train_df_dim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_dim.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Mínimo de relación para separar entre una lista y otra:\n",
    "# CORR_MIN = 0.6\n",
    "# # Crear listas para guardar las características:\n",
    "# high_corr, low_corr = [], []\n",
    "\n",
    "# # Dividir las correlaciones en las listas:\n",
    "# for column in correlation_matrix.columns:\n",
    "#     for index in correlation_matrix.index:\n",
    "#         if index == column:\n",
    "#             continue\n",
    "#         corr = correlation_matrix.loc[index, column]\n",
    "#         if corr >= CORR_MIN or corr <= -CORR_MIN:\n",
    "#             high_corr.append((index, column))\n",
    "#         elif -CORR_MIN < corr < CORR_MIN:\n",
    "#             low_corr.append((index, column))\n",
    "\n",
    "# # Eliminar las correlaciones repetidas:\n",
    "# high_corr = list(set(high_corr))\n",
    "# low_corr = list(set(low_corr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
