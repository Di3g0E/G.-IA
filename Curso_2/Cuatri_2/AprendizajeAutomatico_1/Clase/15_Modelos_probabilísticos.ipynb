{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"WAoQsmu5_SXh"},"outputs":[],"source":["'''\n","URJC / GIA / Aprendizaje Automático 1 / Curso 23-24\n","alfredo.cuesta@urjc.es\n","'''\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import add_dummy_feature\n","from sklearn import linear_model\n","\n","plt.rcParams['figure.figsize']=(5,5)\n","np.set_printoptions(precision=2)\n","pd.set_option(\"display.precision\", 4)"]},{"cell_type":"markdown","metadata":{"id":"IVww-gtG_Ku8"},"source":["# Formulación probabilística de un problema de clasificación\n","\n","$$\n","  y^* = \\mathop{\\arg\\max}\\limits_{y} p\\big(y|{\\bf x}\\big)\n","$$\n","\n","Es decir, buscamos la etiqueta óptima $y^*$, que será la etiqueta más probable para el ejemplo dado.\n","\n","Evidentemente, esta formulación exige construir un modelo probabilístico de la etiqueta o clase condicionado a los ejemplos.\n","\n","Esto se puede hacer de dos modos:\n","1. Estimando directamente $p(y|x)$\n","2. Utilizando modelado bayesiano\n","\n","El primero **ya lo hemos hecho** cuando aprendimos la regresión logística.\n","\n","El segundo se explica en el próximo cuaderno y siguientes."]},{"cell_type":"markdown","metadata":{"id":"kCpsSMJBSIoT"},"source":["# Pérdida de la regresión logística\n","\n","Cuando explicamos la función logística dejamos pendiente aprender una función de pérdida con la que podamos aplicar descenso del gradiente.\n","\n","Desde el punto de vista de un modelo probabilístico, una manera de guiar el ajuste de los parámetros es haciendo que estos maximicen la probabilidad con la que el clasificador toma una decisión para el conjunto de datos de entrenamiento dado.\n","\n","Es decir, para una ejemplo dado ${\\bf x}^{(i)}$ queremos que el clasificador:\n","-  maximice el valor $~p(y=1|{\\bf x}^{(i)})~$ cuando la etiqueta asociada a este ejemplo $y^{(i)}=1$;\n","-  y que maximice el valor $~1-p(y=1|{\\bf x}^{(i)})~$ cuando la etiqueta asociada a este ejemplo $y^{(i)}=0$.\n","\n","Ambas condiciones se pueden reunir en la siguiente expresión:\n","$$\n","\\max \\left( p(y=1|{\\bf x}^{(i)}) ^ {y^{(i)}} \\big(1- p(y=1|{\\bf x}^{(i)}) ^ {1-y^{(i)}}\\big) \\right)\n","$$\n","\n","**¿Por qué?**\n","\n","- $p(y=1|{\\bf x}^{(i)}) ^ {y^{(i)}} $ es la probabilidad con la que el clasificador acertaría al predecir que la etiqueta $y^{(i)}=1$ para este ejemplo $i$.\n","- $1-p(y=1|{\\bf x}^{(i)}) ^ {1-y^{(i)}} $ es la probabilidad con la que el clasificador acertaría al predecir que la etiqueta $y^{(i)}=0$ para este ejemplo $i$.\n","\n","En clasificación binaria NO hay más posibilidades así que el contenido del parentesis que se quiere maximizar es o bien uno o el otro."]},{"cell_type":"markdown","metadata":{"id":"ghgsl5vialvF"},"source":["## Ejemplo\n","\n","Sea el siguiente conjunto de entrenamiento \"de juguete\" compuesto por las columnas $x_{j=1,\\ldots,D}$ de las características junto con la columna $y$ de la etiqueta.\n","\n","Además se ha añadido una columna con la probabilidad estimada por el clasificador de que la etiqueta sea \"1\" para cada ejemplo.\n","\n","| id |$x_1$|$x_2$|$\\ldots$|$x_D$|  $y$ | $p(y=1|x)$ |\n","|:----:|-----:|:-----:|:--------:|:---:|:----:|:----------:|\n","| 1  |$ 0.2$|$0.5$|$\\ldots$|$1$|$1$|$ 0.76 $    |\n","| 2  |$-0.6$|$0.1$|$\\ldots$|$3$|$0$|$ 0.66 $    |\n","| 3  |$ 0.3$|$0.7$|$\\ldots$|$4$|$1$|$ 0.56 $    |\n","| 4  |$ 0.7$|$0.9$|$\\ldots$|$2$|$1$|$ 0.94 $    |\n","| 5  |$-0.3$|$0.3$|$\\ldots$|$1$|$0$|$ 0.16 $    |\n","|$\\vdots$|$\\vdots$|$\\vdots$|$\\vdots$|$\\vdots$|$\\vdots$|$\\vdots$|\n","|$N$ |$-0.2$|$0.1$|$\\ldots$|$5$|$1$|$ 0.76 $    |\n","\n","**Ejemplo 1**\n","El clasificador calcula una probabilidad de 0.76 a la etiqueta correcta, es decir:\n","$$ (0.76)^{(1)} (1-0.76)^{(1-1)} = (0.76)^1 (0.24)^0 = 0.76 $$\n","\n","**Ejemplo 2**\n","El clasificador calcula una probabilidad de 0.34 a la etiqueta correcta, es decir:\n","$$ (0.66)^{(0)} (1-0.66)^{(1-0)} = (0.66)^0 (0.34)^1 = 0.34 $$\n","\n","**Ejemplo 3**\n","El clasificador calcula una probabilidad de 0.56 a la etiqueta correcta, es decir:\n","$$ (0.56)^{(1)} (1-0.56)^{(1-1)} = (0.56)^1 (0.44)^0 = 0.56 $$\n","\n","**Ejemplo 4**\n","El clasificador calcula una probabilidad de 0.94 a la etiqueta correcta, es decir:\n","$$ (0.94)^{(1)} (1-0.94)^{(1-1)} = (0.94)^1 (0.06)^0 = 0.94 $$\n","\n","**Ejemplo 5**\n","El clasificador calcula una probabilidad de 0.84 a la etiqueta correcta, es decir:\n","$$ (0.16)^{(0)} (1-0.16)^{(1-0)} = (0.16)^0 (0.84)^1 = 0.84 $$"]},{"cell_type":"markdown","metadata":{"id":"VlkMqNPMh1ZJ"},"source":["Lo \"ideal\" sería que las probabilidades fueran lo más altas posibles.<br>\n","Por tanto queremos modificar los parámetros en el sentido de MAXIMIZAR estas probabilidades, que surgen de la expresión dada arriba.\n","\n","PERO **NO** podemos maximizar $N$ probabilidades distintas (una por cada ejemplo). Queremos maximizar la probabilidad de **TODO** el conjunto de ejemplos de entrenamiento.\n","\n","Como los ejemplos son iid la probabilidad conjunta de todos ellos es el producto de cada uno. Así que nuestro objetivo es\n","$$\n","\\max \\prod\\limits_{i=1}^{N} \\left( p(y=1|{\\bf x}^{(i)}) ^ {y^{(i)}} \\big(1- p(y=1|{\\bf x}^{(i)}) ^ {1-y^{(i)}}\\big) \\right)\n","$$\n","Podemos cambiar el producto por sumas si tomamos logarítmos,\n","$$\n","\\max \\sum\\limits_{i=1}^{N}  \\log \\left( p(y=1|{\\bf x}^{(i)}) ^ {y^{(i)}} \\big(1- p(y=1|{\\bf x}^{(i)}) ^ {1-y^{(i)}}\\big) \\right)\n","$$\n","Si en vez de maximizar queremos minimizar (por ejemplo porque tenemos un algoritmo de descenso de gradiente ya programado), entonces basta con cambiar el signo\n","$$\n","\\min \\sum\\limits_{i=1}^{N}  -\\log \\left( p(y=1|{\\bf x}^{(i)}) ^ {y^{(i)}} \\big(1- p(y=1|{\\bf x}^{(i)}) ^ {1-y^{(i)}}\\big) \\right)\n","$$\n","Finalmente, manipulando con el logaritmo y el parentesis llegamos a\n","$$\n","\\min \\sum\\limits_{i=1}^{N}  \n","- {y^{(i)}} \\log \\big( p(y=1|{\\bf x}^{(i)}) \\big)\n","- ({1-y^{(i)}}) \\log \\big( 1- p(y=1|{\\bf x}^{(i)}) \\big)\n","$$"]},{"cell_type":"markdown","metadata":{"id":"EsfxWUutlWLd"},"source":["En definitiva, como la regresión logística devuelve directamente $~p(y=1|{\\bf x}^{(i)}; {\\bf w})~$, podemos utilizar la siguiente función de pérdida\n","$$\n","\\mathcal{L}_{\\rm log} = \\sum\\limits_{i=1}^{N}  \n","- {y^{(i)}} \\log \\big( p(y=1|{\\bf x}^{(i)}) \\big)\n","- ({1-y^{(i)}}) \\log \\big( 1- p(y=1|{\\bf x}^{(i)}) \\big),\n","$$\n","\n","llamada **log-loss**, para encontrar los parámetros mediante descenso del gradiente.\n","\n","Por supuesto, a esta pérdida se le pueden añadir términos de regularización L1, L2 o ElasticNet.\n","\n","Lo interesante de esta pérdida es que tiene una interpretación probabilística.\n","\n","**Esta función de pérdida busca maximizar la probabilidad de las etiquetas dados los datos.**\n","\n","> _Recuerda que hasta este momento, la manera de aprender los parámetros de una clasificador lineal era mediante pérdidas típicas de regresión como MSE o MAE, y después aplicabamos una función logística para obtener estimaciones de probabilidad._"]},{"cell_type":"markdown","metadata":{"id":"6ij3i3AJ_R2O"},"source":["# La entropía cruzada como función de pérdida.\n","En un problema de clasificación binario podemos interpretar la etiqueta verdadera (0 o 1) como la probabilidad de que el ejemplo pertenezca a la clase \"positiva\"; es decir:\n","- si $~y^{(i)}=0~$ quiere decir que la probabilidad de que el ejemplo $~{\\bf x}^{(i)}~$ sea _positivo_ es **NULA**\n","- si $~y^{(i)}=1~$ quiere decir que la probabilidad de que el ejemplo $~{\\bf x}^{(i)}~$ sea _positivo_ es **TOTAL**\n","\n","Por otro lado, un modelo que estima $~p(y|x)~$ **NO** es tan _taxativo_; es decir que seguramente devuelve un valor en el intervalo $~(0,1).$\n","\n","Con el siguiente código podemos representar las distribuciones de probabildad verdadera y estimada de un cierto ejemplo.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":162},"executionInfo":{"elapsed":1149,"status":"ok","timestamp":1710741230359,"user":{"displayName":"alfredo cuesta infante Universidad Rey Juan Carlos","userId":"17488335604138000921"},"user_tz":-60},"id":"-7BEH6eYE6Qi","outputId":"a2872c53-b133-46a6-d95d-6029b1f22a47"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 500x100 with 2 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["Yi_true = 0    #<- 0 , 1\n","Yi_pred = 0.3  #<- (0,1)\n","\n","p_YgivenX_true = [1-Yi_true, Yi_true]\n","p_YgivenX_pred = [1-Yi_pred, Yi_pred]\n","\n","plt.rcParams['figure.figsize']=(5,1)\n","plt.subplot(121)\n","plt.bar([0,1],p_YgivenX_true, width=0.3, tick_label=[0,1])\n","plt.title('$p(y|x)$ real')\n","plt.axis([-.5, 1.5, 0, 1.1])\n","plt.subplot(122)\n","plt.bar([0,1],p_YgivenX_pred, width=0.3, tick_label=[0,1])\n","plt.title('$p(y|x)$ estimada')\n","plt.axis([-.5, 1.5, 0, 1.1])\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"wYPGUiCEHigE"},"source":["¿ Existe alguna función de pérdida que **compare** ambas distribuciones y guíe el descenso de gradiente?"]},{"cell_type":"markdown","metadata":{"id":"C_pYb8JgBHJZ"},"source":["$\\fbox{ Sí }$  la **entropía cruzada**\n"]},{"cell_type":"markdown","metadata":{"id":"eLpYol0Y7NQG"},"source":["## Definición\n","La Entropía cruzada entre la distribución verdadera $p$ y la distribución estimada $\\hat p$ es el valor esperado respecto a la distribución $p$ del $-\\log \\hat p$, es decir:\n","$$\n","\\begin{align}\n","{\\rm CE}(p,\\hat p) &=  -\\mathbb{E}_p \\left[ \\log \\hat p \\right] \\\\\n","&= -\\sum\\limits_{\\forall y} p(y|x) \\log \\hat p(y|x)\n","\\end{align}\n","$$\n","\n","Para problemas de clasificación binaria $y=\\{0,1\\}$, y eliminando la notación de probabilidad condicionada para tener una expresión más sencilla, la entropía cruzada \"binaria\" (_Binary Cross Entropy_, BCE), se reduce a\n","$$\n","\\begin{align}\n","{\\rm BCE}(p,\\hat p)\n","&=\n","p(y=1)(-\\log \\hat p(y=1)) +\n","p(y=0)(-\\log \\hat p(y=0)) \\\\\n","&=\n","-p(y=1)(\\log \\hat p(y=1))\n","-(1-p(y=1))(\\log \\hat p(y=0))\n","\\\\  \n","\\end{align}\n","$$\n","que coincide con la log-loss para un único ejemplo. La extensión a $N$ es simplemente la suma de todas ellas.\n"]},{"cell_type":"markdown","metadata":{"id":"SjPCe1vVHLzy"},"source":["# Resumen.\n","\n","Se puede dar una interpretación probabilística desde varios puntos de vista.\n","\n","1. La etiqueta estimada es la más probable<br>\n","$\\rightarrow$ Planteamos la pérdida Log-loss.\n","2. La distribución de probabilidad estimada sobre las etiquetas debe ser lo más parecida posible a la distribución de probabilidad real<br>\n","$\\rightarrow$ Planteamos la pérdida _Entropía cruzada_\n","\n","$\\fbox{Para problemas de clasificación binaria ambas coinciden}$\n","\n","3. La etiqueta estimada es la más probable \"a posteriori\" (MAP)<br>\n","$\\rightarrow$ Este es el planteamiento **Bayesiano**, que veremos en el siguiente cuaderno"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KbuqYqMHIi5B"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOcHfQCMg+aTFqt0WNCd2Tw","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
